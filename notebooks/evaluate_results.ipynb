{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedNAMs+ Results Evaluation and Visualization\n",
    "\n",
    "This notebook provides detailed analysis and visualization of FedNAMs+ experiment results.\n",
    "\n",
    "## Contents\n",
    "- Load experiment results\n",
    "- Analyze classification performance\n",
    "- Examine explanation quality\n",
    "- Evaluate uncertainty quantification\n",
    "- Generate publication-ready figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Setup plotting\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify experiment directory\n",
    "experiment_dir = Path('outputs/fednams_colab_demo')  # Update this path\n",
    "\n",
    "# Load results\n",
    "with open(experiment_dir / 'results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded results from: {experiment_dir}\")\n",
    "print(f\"Experiment: {results.get('experiment_name', 'N/A')}\")\n",
    "print(f\"Completed rounds: {len(results.get('training_history', {}).get('rounds', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test metrics\n",
    "test_metrics = results['test_metrics']\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR'],\n",
    "    'Value': [\n",
    "        test_metrics.get('accuracy', 0),\n",
    "        test_metrics.get('precision', 0),\n",
    "        test_metrics.get('recall', 0),\n",
    "        test_metrics.get('f1', 0),\n",
    "        test_metrics.get('auc_roc', 0),\n",
    "        test_metrics.get('auc_pr', 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "display(metrics_df.style.format({'Value': '{:.4f}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(metrics_df['Metric'], metrics_df['Value'], color='steelblue')\n",
    "ax.set_xlabel('Score')\n",
    "ax.set_title('FedNAMs+ Test Set Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width:.3f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / 'test_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-class metrics if available\n",
    "if 'per_class_metrics' in test_metrics:\n",
    "    per_class = test_metrics['per_class_metrics']\n",
    "    \n",
    "    # Create dataframe\n",
    "    class_names = per_class.get('class_names', [f'Class {i}' for i in range(len(per_class['f1']))])\n",
    "    per_class_df = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': per_class['precision'],\n",
    "        'Recall': per_class['recall'],\n",
    "        'F1-Score': per_class['f1'],\n",
    "        'AUC-ROC': per_class.get('auc_roc', [0]*len(class_names))\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Per-Class Performance ===\")\n",
    "    display(per_class_df.style.format({\n",
    "        'Precision': '{:.3f}',\n",
    "        'Recall': '{:.3f}',\n",
    "        'F1-Score': '{:.3f}',\n",
    "        'AUC-ROC': '{:.3f}'\n",
    "    }))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    ax.bar(x - 1.5*width, per_class_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "    ax.bar(x - 0.5*width, per_class_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "    ax.bar(x + 0.5*width, per_class_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "    ax.bar(x + 1.5*width, per_class_df['AUC-ROC'], width, label='AUC-ROC', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Per-class metrics not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Progress Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training history\n",
    "history = results['training_history']\n",
    "\n",
    "# Create comprehensive training plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['rounds'], history['train_loss'], 'o-', label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history['rounds'], history['val_loss'], 's-', label='Validation', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Round')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['rounds'], history['train_accuracy'], 'o-', label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history['rounds'], history['val_accuracy'], 's-', label='Validation', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Round')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "if 'train_f1' in history:\n",
    "    axes[1, 0].plot(history['rounds'], history['train_f1'], 'o-', label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(history['rounds'], history['val_f1'], 's-', label='Validation', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Round')\n",
    "    axes[1, 0].set_ylabel('F1-Score')\n",
    "    axes[1, 0].set_title('Training and Validation F1-Score', fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC-ROC\n",
    "if 'train_auc' in history:\n",
    "    axes[1, 1].plot(history['rounds'], history['train_auc'], 'o-', label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(history['rounds'], history['val_auc'], 's-', label='Validation', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Round')\n",
    "    axes[1, 1].set_ylabel('AUC-ROC')\n",
    "    axes[1, 1].set_title('Training and Validation AUC-ROC', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / 'training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explanation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load explanation metrics\n",
    "if 'explanation_metrics' in results:\n",
    "    exp_metrics = results['explanation_metrics']\n",
    "    \n",
    "    print(\"\\n=== Explanation Quality Metrics ===\")\n",
    "    print(f\"SHAP Consistency: {exp_metrics.get('shap_consistency', 0):.4f}\")\n",
    "    print(f\"Feature Stability: {exp_metrics.get('feature_stability', 0):.4f}\")\n",
    "    print(f\"Cross-Client Agreement: {exp_metrics.get('cross_client_agreement', 0):.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    exp_df = pd.DataFrame({\n",
    "        'Metric': ['SHAP Consistency', 'Feature Stability', 'Cross-Client Agreement'],\n",
    "        'Score': [\n",
    "            exp_metrics.get('shap_consistency', 0),\n",
    "            exp_metrics.get('feature_stability', 0),\n",
    "            exp_metrics.get('cross_client_agreement', 0)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    bars = ax.bar(exp_df['Metric'], exp_df['Score'], color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Explanation Quality Metrics', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'explanation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Explanation metrics not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SHAP plots\n",
    "shap_dir = experiment_dir / 'shap_visualizations'\n",
    "\n",
    "if shap_dir.exists():\n",
    "    print(\"\\n=== SHAP Visualizations ===\")\n",
    "    \n",
    "    # Summary plot\n",
    "    summary_path = shap_dir / 'shap_summary.png'\n",
    "    if summary_path.exists():\n",
    "        print(\"\\nSHAP Summary Plot:\")\n",
    "        display(Image(filename=str(summary_path)))\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_path = shap_dir / 'feature_importance.png'\n",
    "    if importance_path.exists():\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        display(Image(filename=str(importance_path)))\n",
    "    \n",
    "    # Client comparison\n",
    "    comparison_path = shap_dir / 'client_comparison.png'\n",
    "    if comparison_path.exists():\n",
    "        print(\"\\nCross-Client Feature Comparison:\")\n",
    "        display(Image(filename=str(comparison_path)))\n",
    "else:\n",
    "    print(\"SHAP visualizations not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Uncertainty Quantification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uncertainty metrics\n",
    "if 'uncertainty_metrics' in results:\n",
    "    unc_metrics = results['uncertainty_metrics']\n",
    "    \n",
    "    print(\"\\n=== Uncertainty Quantification Metrics ===\")\n",
    "    print(f\"Coverage: {unc_metrics.get('coverage', 0):.4f}\")\n",
    "    print(f\"Target Confidence: {unc_metrics.get('target_confidence', 0.9):.2f}\")\n",
    "    print(f\"Average Set Size: {unc_metrics.get('avg_set_size', 0):.2f}\")\n",
    "    print(f\"Median Set Size: {unc_metrics.get('median_set_size', 0):.2f}\")\n",
    "    \n",
    "    # Visualize coverage\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Coverage comparison\n",
    "    coverage_data = {\n",
    "        'Metric': ['Target', 'Achieved'],\n",
    "        'Coverage': [\n",
    "            unc_metrics.get('target_confidence', 0.9),\n",
    "            unc_metrics.get('coverage', 0)\n",
    "        ]\n",
    "    }\n",
    "    axes[0].bar(coverage_data['Metric'], coverage_data['Coverage'], \n",
    "                color=['#95a5a6', '#2ecc71'])\n",
    "    axes[0].set_ylabel('Coverage')\n",
    "    axes[0].set_title('Coverage: Target vs Achieved', fontweight='bold')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Set size distribution\n",
    "    if 'set_size_distribution' in unc_metrics:\n",
    "        set_sizes = unc_metrics['set_size_distribution']\n",
    "        axes[1].hist(set_sizes, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[1].axvline(unc_metrics.get('avg_set_size', 0), \n",
    "                       color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        axes[1].axvline(unc_metrics.get('median_set_size', 0), \n",
    "                       color='green', linestyle='--', linewidth=2, label='Median')\n",
    "        axes[1].set_xlabel('Prediction Set Size')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Prediction Set Size Distribution', fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'uncertainty_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Uncertainty metrics not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Communication Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load communication metrics\n",
    "if 'communication_metrics' in results:\n",
    "    comm_metrics = results['communication_metrics']\n",
    "    \n",
    "    print(\"\\n=== Communication Cost ===\")\n",
    "    print(f\"Parameters per round: {comm_metrics.get('params_per_round', 0):,}\")\n",
    "    print(f\"MB per round: {comm_metrics.get('mb_per_round', 0):.2f}\")\n",
    "    print(f\"Total rounds: {comm_metrics.get('total_rounds', 0)}\")\n",
    "    print(f\"Total communication: {comm_metrics.get('total_mb', 0):.2f} MB\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    rounds = list(range(1, comm_metrics.get('total_rounds', 0) + 1))\n",
    "    cumulative_mb = [i * comm_metrics.get('mb_per_round', 0) for i in rounds]\n",
    "    \n",
    "    ax.plot(rounds, cumulative_mb, 'o-', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Round')\n",
    "    ax.set_ylabel('Cumulative Communication (MB)')\n",
    "    ax.set_title('Communication Cost Over Training', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(experiment_dir / 'communication_cost.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Communication metrics not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = f\"\"\"\n",
    "{'='*60}\n",
    "FedNAMs+ Experiment Summary\n",
    "{'='*60}\n",
    "\n",
    "Experiment: {results.get('experiment_name', 'N/A')}\n",
    "Date: {results.get('timestamp', 'N/A')}\n",
    "\n",
    "CLASSIFICATION PERFORMANCE\n",
    "{'-'*60}\n",
    "Accuracy:  {test_metrics.get('accuracy', 0):.4f}\n",
    "F1-Score:  {test_metrics.get('f1', 0):.4f}\n",
    "AUC-ROC:   {test_metrics.get('auc_roc', 0):.4f}\n",
    "AUC-PR:    {test_metrics.get('auc_pr', 0):.4f}\n",
    "\"\"\"\n",
    "\n",
    "if 'explanation_metrics' in results:\n",
    "    exp_metrics = results['explanation_metrics']\n",
    "    summary += f\"\"\"\n",
    "EXPLANATION QUALITY\n",
    "{'-'*60}\n",
    "SHAP Consistency:        {exp_metrics.get('shap_consistency', 0):.4f}\n",
    "Feature Stability:       {exp_metrics.get('feature_stability', 0):.4f}\n",
    "Cross-Client Agreement:  {exp_metrics.get('cross_client_agreement', 0):.4f}\n",
    "\"\"\"\n",
    "\n",
    "if 'uncertainty_metrics' in results:\n",
    "    unc_metrics = results['uncertainty_metrics']\n",
    "    summary += f\"\"\"\n",
    "UNCERTAINTY QUANTIFICATION\n",
    "{'-'*60}\n",
    "Coverage:           {unc_metrics.get('coverage', 0):.4f}\n",
    "Target Confidence:  {unc_metrics.get('target_confidence', 0.9):.2f}\n",
    "Avg Set Size:       {unc_metrics.get('avg_set_size', 0):.2f}\n",
    "\"\"\"\n",
    "\n",
    "if 'communication_metrics' in results:\n",
    "    comm_metrics = results['communication_metrics']\n",
    "    summary += f\"\"\"\n",
    "COMMUNICATION COST\n",
    "{'-'*60}\n",
    "MB per round:        {comm_metrics.get('mb_per_round', 0):.2f}\n",
    "Total communication: {comm_metrics.get('total_mb', 0):.2f} MB\n",
    "\"\"\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "{'='*60}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(experiment_dir / 'summary_report.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nâœ“ Summary report saved to: {experiment_dir / 'summary_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive analysis of FedNAMs+ results including:\n",
    "- Classification performance metrics\n",
    "- Per-class analysis\n",
    "- Training progress visualization\n",
    "- Explanation quality assessment\n",
    "- SHAP visualizations\n",
    "- Uncertainty quantification analysis\n",
    "- Communication cost tracking\n",
    "\n",
    "All visualizations have been saved to the experiment directory for publication use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
