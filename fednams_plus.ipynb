{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# === CELL 1: Mount Drive and Setup ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "# Clone repo if not already there\n",
        "if not os.path.exists('fednams-plus'):\n",
        "    !git clone https://github.com/siddharth10ss/fednams-plus.git\n",
        "\n",
        "%cd fednams-plus\n",
        "!pip install -q torch torchvision pandas numpy pillow\n",
        "\n",
        "print(\"âœ“ Setup complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD_D7GNZI3N8",
        "outputId": "74767aa8-830d-4887-e90f-92ba4b4e260d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'fednams-plus'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 32 (delta 0), reused 32 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 26.26 KiB | 8.75 MiB/s, done.\n",
            "/content/fednams-plus\n",
            "âœ“ Setup complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OPTION 3: Extract Subset of NIH Dataset (10,000 images)\n",
        "# ============================================================================\n",
        "\n",
        "# === CELL 1: Clean up partial extraction ===\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# First, clean up any partial extraction\n",
        "partial_dirs = [\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_001',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_002',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_003',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_004',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_005',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_006',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_007',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_008',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_009',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_010',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_011',\n",
        "    '/content/drive/MyDrive/NIH Chest XRAY Dataset/images_012',\n",
        "]\n",
        "\n",
        "print(\"ðŸ§¹ Cleaning up partial extraction...\")\n",
        "for dir_path in partial_dirs:\n",
        "    if Path(dir_path).exists():\n",
        "        print(f\"  Removing {Path(dir_path).name}...\")\n",
        "        shutil.rmtree(dir_path)\n",
        "\n",
        "print(\"âœ“ Cleanup complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwlBgyQ-XZiL",
        "outputId": "39fd802d-a719-4dc2-a6f0-25818d665456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¹ Cleaning up partial extraction...\n",
            "  Removing images_006...\n",
            "  Removing images_007...\n",
            "  Removing images_008...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CELL 2: Extract Subset ===\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/NIH Chest XRAY Dataset/archive.zip'\n",
        "extract_dir = '/content/drive/MyDrive/NIH Chest XRAY Dataset/'\n",
        "\n",
        "print(\"ðŸ“¦ Extracting subset of NIH dataset...\")\n",
        "print(\"ðŸ“Š Target: 10,000 images + CSV files\")\n",
        "print(\"â³ This will take 5-10 minutes...\\n\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    all_files = zip_ref.namelist()\n",
        "\n",
        "    # Separate files by type\n",
        "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
        "    pdf_files = [f for f in all_files if f.endswith('.pdf')]\n",
        "    image_files = [f for f in all_files if f.endswith('.png')]\n",
        "\n",
        "    print(f\"ðŸ“ Found in archive:\")\n",
        "    print(f\"  - CSV files: {len(csv_files)}\")\n",
        "    print(f\"  - PDF files: {len(pdf_files)}\")\n",
        "    print(f\"  - Image files: {len(image_files)}\")\n",
        "\n",
        "    # Select files to extract\n",
        "    files_to_extract = []\n",
        "    files_to_extract.extend(csv_files)  # All CSV files\n",
        "    files_to_extract.extend(pdf_files)  # All PDF files\n",
        "    files_to_extract.extend(image_files[:10000])  # First 10,000 images\n",
        "\n",
        "    print(f\"\\nðŸ“¦ Extracting {len(files_to_extract)} files...\")\n",
        "    print(f\"  - CSV/PDF: {len(csv_files) + len(pdf_files)}\")\n",
        "    print(f\"  - Images: 10,000\")\n",
        "\n",
        "    # Extract with progress\n",
        "    for i, file in enumerate(files_to_extract, 1):\n",
        "        zip_ref.extract(file, extract_dir)\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            progress = (i / len(files_to_extract)) * 100\n",
        "            print(f\"  Progress: {i}/{len(files_to_extract)} ({progress:.1f}%)\")\n",
        "\n",
        "print(\"\\nâœ“ Extraction complete!\")\n"
      ],
      "metadata": {
        "id": "GlXz52sYhPst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc7e11f-dcfb-4dc7-d980-05c40af6da14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Extracting subset of NIH dataset...\n",
            "ðŸ“Š Target: 10,000 images + CSV files\n",
            "â³ This will take 5-10 minutes...\n",
            "\n",
            "ðŸ“ Found in archive:\n",
            "  - CSV files: 2\n",
            "  - PDF files: 4\n",
            "  - Image files: 112120\n",
            "\n",
            "ðŸ“¦ Extracting 10006 files...\n",
            "  - CSV/PDF: 6\n",
            "  - Images: 10,000\n",
            "  Progress: 500/10006 (5.0%)\n",
            "  Progress: 1000/10006 (10.0%)\n",
            "  Progress: 1500/10006 (15.0%)\n",
            "  Progress: 2000/10006 (20.0%)\n",
            "  Progress: 2500/10006 (25.0%)\n",
            "  Progress: 3000/10006 (30.0%)\n",
            "  Progress: 3500/10006 (35.0%)\n",
            "  Progress: 4000/10006 (40.0%)\n",
            "  Progress: 4500/10006 (45.0%)\n",
            "  Progress: 5000/10006 (50.0%)\n",
            "  Progress: 5500/10006 (55.0%)\n",
            "  Progress: 6000/10006 (60.0%)\n",
            "  Progress: 6500/10006 (65.0%)\n",
            "  Progress: 7000/10006 (70.0%)\n",
            "  Progress: 7500/10006 (75.0%)\n",
            "  Progress: 8000/10006 (80.0%)\n",
            "  Progress: 8500/10006 (84.9%)\n",
            "  Progress: 9000/10006 (89.9%)\n",
            "  Progress: 9500/10006 (94.9%)\n",
            "  Progress: 10000/10006 (99.9%)\n",
            "\n",
            "âœ“ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CELL 3: Verify Extraction ===\n",
        "import pandas as pd\n",
        "\n",
        "data_dir = Path('/content/drive/MyDrive/NIH Chest XRAY Dataset')\n",
        "\n",
        "print(\"\\nðŸ“ Verifying extracted files...\")\n",
        "\n",
        "# Check CSV files\n",
        "csv_files = list(data_dir.glob('*.csv'))\n",
        "print(f\"\\nâœ“ CSV files: {len(csv_files)}\")\n",
        "for csv in csv_files:\n",
        "    print(f\"  - {csv.name}\")\n",
        "\n",
        "# Check image directories\n",
        "image_dirs = [d for d in data_dir.glob('images*') if d.is_dir()]\n",
        "print(f\"\\nâœ“ Image directories: {len(image_dirs)}\")\n",
        "\n",
        "# Count images\n",
        "total_images = 0\n",
        "for img_dir in image_dirs:\n",
        "    count = len(list(img_dir.rglob('*.png')))\n",
        "    total_images += count\n",
        "    print(f\"  - {img_dir.name}: {count} images\")\n",
        "\n",
        "print(f\"\\nâœ“ Total images extracted: {total_images}\")\n",
        "\n",
        "# Load labels\n",
        "labels_file = data_dir / 'Data_Entry_2017.csv'\n",
        "if labels_file.exists():\n",
        "    labels_df = pd.read_csv(labels_file)\n",
        "    print(f\"\\nâœ“ Labels file loaded: {len(labels_df)} total entries\")\n",
        "    print(f\"  (We have images for first {total_images} entries)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… SUBSET EXTRACTION SUCCESSFUL!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nðŸ“Š What you have:\")\n",
        "print(f\"  âœ“ 10,000 chest X-ray images\")\n",
        "print(f\"  âœ“ Complete labels CSV (all 112k entries)\")\n",
        "print(f\"  âœ“ Bounding box CSV\")\n",
        "print(f\"  âœ“ README PDF\")\n",
        "print(f\"\\nðŸ’¡ This is plenty for:\")\n",
        "print(f\"  - Development and testing\")\n",
        "print(f\"  - Training federated models\")\n",
        "print(f\"  - Validating the complete pipeline\")\n",
        "print(f\"\\nðŸ“ You can extract more images later if needed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSi5BfLSuIk0",
        "outputId": "4796eb04-7444-499e-9f63-6167e4e02963"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“ Verifying extracted files...\n",
            "\n",
            "âœ“ CSV files: 2\n",
            "  - BBox_List_2017.csv\n",
            "  - Data_Entry_2017.csv\n",
            "\n",
            "âœ“ Image directories: 2\n",
            "  - images_001: 4999 images\n",
            "  - images_002: 5001 images\n",
            "\n",
            "âœ“ Total images extracted: 10000\n",
            "\n",
            "âœ“ Labels file loaded: 112120 total entries\n",
            "  (We have images for first 10000 entries)\n",
            "\n",
            "============================================================\n",
            "âœ… SUBSET EXTRACTION SUCCESSFUL!\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š What you have:\n",
            "  âœ“ 10,000 chest X-ray images\n",
            "  âœ“ Complete labels CSV (all 112k entries)\n",
            "  âœ“ Bounding box CSV\n",
            "  âœ“ README PDF\n",
            "\n",
            "ðŸ’¡ This is plenty for:\n",
            "  - Development and testing\n",
            "  - Training federated models\n",
            "  - Validating the complete pipeline\n",
            "\n",
            "ðŸ“ You can extract more images later if needed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Pull latest code with data module ===\n",
        "%cd /content/fednams-plus\n",
        "!git pull origin main\n",
        "\n",
        "print(\"\\nðŸ“ Checking data module...\")\n",
        "!ls -la data/\n",
        "\n",
        "print(\"\\nâœ“ Data module should now be available!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCY43SWQwf_d",
        "outputId": "458ef3e8-f9c7-4aa5-9bd9-6dab7cc57b57"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fednams-plus\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 2), reused 10 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (10/10), 10.07 KiB | 1.26 MiB/s, done.\n",
            "From https://github.com/siddharth10ss/fednams-plus\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   ffd5871..43f24c0  main       -> origin/main\n",
            "Updating ffd5871..43f24c0\n",
            "Fast-forward\n",
            " .gitignore           |  18 \u001b[32m+\u001b[m\u001b[31m--\u001b[m\n",
            " data/README.md       | 135 \u001b[32m++++++++++++++++++++++\u001b[m\n",
            " data/__init__.py     |  18 \u001b[32m+++\u001b[m\n",
            " data/base_dataset.py | 151 \u001b[32m++++++++++++++++++++++++\u001b[m\n",
            " data/downloader.py   | 164 \u001b[32m++++++++++++++++++++++++++\u001b[m\n",
            " data/partitioner.py  | 320 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " data/preprocessor.py | 246 \u001b[32m+++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 7 files changed, 1045 insertions(+), 7 deletions(-)\n",
            " create mode 100644 data/README.md\n",
            " create mode 100644 data/__init__.py\n",
            " create mode 100644 data/base_dataset.py\n",
            " create mode 100644 data/downloader.py\n",
            " create mode 100644 data/partitioner.py\n",
            " create mode 100644 data/preprocessor.py\n",
            "\n",
            "ðŸ“ Checking data module...\n",
            "total 52\n",
            "drwxr-xr-x  2 root root  4096 Nov 12 15:49 .\n",
            "drwxr-xr-x 14 root root  4096 Nov 12 15:49 ..\n",
            "-rw-r--r--  1 root root  4115 Nov 12 15:49 base_dataset.py\n",
            "-rw-r--r--  1 root root  5345 Nov 12 15:49 downloader.py\n",
            "-rw-r--r--  1 root root   487 Nov 12 15:49 __init__.py\n",
            "-rw-r--r--  1 root root 11588 Nov 12 15:49 partitioner.py\n",
            "-rw-r--r--  1 root root  8025 Nov 12 15:49 preprocessor.py\n",
            "-rw-r--r--  1 root root  3116 Nov 12 15:49 README.md\n",
            "\n",
            "âœ“ Data module should now be available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Pull the fix and re-run ===\n",
        "%cd /content/fednams-plus\n",
        "!git pull origin main\n",
        "\n",
        "# Re-import the fixed module\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Remove old module from cache\n",
        "if 'data.partitioner' in sys.modules:\n",
        "    del sys.modules['data.partitioner']\n",
        "if 'data' in sys.modules:\n",
        "    del sys.modules['data']\n",
        "\n",
        "# Re-import\n",
        "from data import FederatedDataPartitioner\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"âœ“ Updated module imported\")\n",
        "\n",
        "# Now generate statistics (this should work)\n",
        "print(\"\\nðŸ“Š Generating statistics...\")\n",
        "\n",
        "stats_dir = Path('/content/drive/MyDrive/NIH Chest XRAY Dataset/outputs/partitions')\n",
        "stats = partitioner.generate_statistics(client_datasets, stats_dir)\n",
        "\n",
        "print(\"âœ“ Statistics saved to Google Drive!\")\n",
        "print(f\"\\nPartition Statistics:\")\n",
        "print(stats)\n",
        "\n",
        "# Test all strategies\n",
        "print(f\"\\nðŸ§ª Testing all partitioning strategies...\")\n",
        "\n",
        "strategies = {\n",
        "    'dirichlet': {'alpha': 0.5},\n",
        "    'pathology': {},\n",
        "    'quantity': {}\n",
        "}\n",
        "\n",
        "for strategy_name, kwargs in strategies.items():\n",
        "    print(f\"\\n--- {strategy_name.capitalize()} Strategy ---\")\n",
        "\n",
        "    part = FederatedDataPartitioner(\n",
        "        num_clients=5,\n",
        "        strategy=strategy_name,\n",
        "        min_samples=200,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    clients = part.partition(dataset)\n",
        "\n",
        "    for cid, subset in clients.items():\n",
        "        print(f\"  Client {cid}: {len(subset)} samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… TASK 2: DATA MODULE - FULLY VALIDATED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nðŸ“Š Complete Test Summary:\")\n",
        "print(f\"  âœ“ Dataset: NIH Chest X-rays\")\n",
        "print(f\"  âœ“ Images extracted: 10,000\")\n",
        "print(f\"  âœ“ Test dataset: 5,000 samples\")\n",
        "print(f\"  âœ“ Image size: 1024x1024 â†’ 224x224\")\n",
        "print(f\"  âœ“ Classes: 15 (14 diseases + No findings)\")\n",
        "print(f\"  âœ“ Federated clients: 5\")\n",
        "print(f\"\\nâœ… All Components Validated:\")\n",
        "print(f\"  âœ“ DataDownloader (manual upload)\")\n",
        "print(f\"  âœ“ DataPreprocessor (transforms, augmentation)\")\n",
        "print(f\"  âœ“ MIMICCXRDataset (lazy loading)\")\n",
        "print(f\"  âœ“ FederatedDataPartitioner (3 strategies)\")\n",
        "print(f\"  âœ“ Statistics generation (fixed!)\")\n",
        "print(f\"\\nðŸ“ READY FOR TASK 3: BUILD NAM MODELS!\")\n",
        "print(f\"ðŸ’¾ Data persists in Google Drive\")\n",
        "print(f\"ðŸŽ‰ Task 2 Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "qlFKKYT5v4nY",
        "outputId": "6f9e9c76-0c93-4469-a31b-1fbf1f27eb9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fednams-plus\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 437 bytes | 145.00 KiB/s, done.\n",
            "From https://github.com/siddharth10ss/fednams-plus\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   43f24c0..8cb0e70  main       -> origin/main\n",
            "Updating 43f24c0..8cb0e70\n",
            "Fast-forward\n",
            " data/partitioner.py | 6 \u001b[32m+++\u001b[m\u001b[31m---\u001b[m\n",
            " 1 file changed, 3 insertions(+), 3 deletions(-)\n",
            "âœ“ Updated module imported\n",
            "\n",
            "ðŸ“Š Generating statistics...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type float32 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1493077766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mstats_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/NIH Chest XRAY Dataset/outputs/partitions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ“ Statistics saved to Google Drive!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fednams-plus/data/partitioner.py\u001b[0m in \u001b[0;36mgenerate_statistics\u001b[0;34m(self, partitions, output_dir)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mclient_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'client_{client_id}_metadata.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Create DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Workaround: Reload module without restart ===\n",
        "%cd /content/fednams-plus\n",
        "!git pull origin main\n",
        "\n",
        "# Force reload\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# Remove all data module variants from cache\n",
        "modules_to_remove = [k for k in sys.modules.keys() if k.startswith('data')]\n",
        "for mod in modules_to_remove:\n",
        "    del sys.modules[mod]\n",
        "\n",
        "# Re-import fresh\n",
        "from data import FederatedDataPartitioner\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"âœ“ Module reloaded with fix\")\n",
        "\n",
        "# Create NEW partitioner instance\n",
        "partitioner_new = FederatedDataPartitioner(\n",
        "    num_clients=5,\n",
        "    strategy='dirichlet',\n",
        "    alpha=0.5,\n",
        "    min_samples=200\n",
        ")\n",
        "\n",
        "# Re-partition\n",
        "client_datasets_new = partitioner_new.partition(dataset)\n",
        "\n",
        "print(\"\\nðŸ“Š Generating statistics with fixed code...\")\n",
        "stats_dir = Path('/content/drive/MyDrive/NIH Chest XRAY Dataset/outputs/partitions')\n",
        "stats = partitioner_new.generate_statistics(client_datasets_new, stats_dir)\n",
        "\n",
        "print(\"âœ“ Statistics saved!\")\n",
        "print(stats)\n",
        "\n",
        "print(\"\\nâœ… TASK 2 COMPLETE!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ds9_T-4yEUE",
        "outputId": "13c41e48-e193-4453-af99-15bdc4e686bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fednams-plus\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 452 bytes | 64.00 KiB/s, done.\n",
            "From https://github.com/siddharth10ss/fednams-plus\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   8cb0e70..2030c69  main       -> origin/main\n",
            "Updating 8cb0e70..2030c69\n",
            "Fast-forward\n",
            " data/partitioner.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
            "âœ“ Module reloaded with fix\n",
            "\n",
            "ðŸ“Š Generating statistics with fixed code...\n",
            "âœ“ Statistics saved!\n",
            "   client_id  num_samples                                 label_distribution  \\\n",
            "0          0          383  [50.0, 21.0, 146.0, 266.0, 168.0, 204.0, 180.0...   \n",
            "1          1          891  [606.0, 525.0, 471.0, 442.0, 446.0, 422.0, 434...   \n",
            "2          2         2434  [1674.0, 1466.0, 1184.0, 1189.0, 1218.0, 1213....   \n",
            "3          3          448  [27.0, 186.0, 79.0, 232.0, 242.0, 226.0, 225.0...   \n",
            "4          4          844  [126.0, 235.0, 584.0, 401.0, 404.0, 432.0, 415...   \n",
            "\n",
            "   avg_labels_per_sample  \n",
            "0               6.738904  \n",
            "1               7.748597  \n",
            "2               7.762531  \n",
            "3               6.727679  \n",
            "4               7.090047  \n",
            "\n",
            "âœ… TASK 2 COMPLETE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Test Task 3: Models Module ===\n",
        "%cd /content/fednams-plus\n",
        "!git pull origin main\n",
        "\n",
        "import torch\n",
        "from models import FedNAMsModel, FedAvgCNN, CentralizedNAM\n",
        "from configs import ModelConfig\n",
        "\n",
        "print(\"ðŸ§ª Testing Models Module...\\n\")\n",
        "\n",
        "# Create config\n",
        "config = ModelConfig(\n",
        "    backbone='resnet18',\n",
        "    pretrained=True,\n",
        "    feature_dim=512,\n",
        "    num_classes=15,  # NIH has 15 classes\n",
        "    nam_hidden_units=[64, 32],\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "# Test FedNAMsModel\n",
        "print(\"1. Testing FedNAMsModel...\")\n",
        "model = FedNAMsModel(config)\n",
        "print(f\"âœ“ Model created\")\n",
        "print(f\"  Parameters: {model.get_num_parameters():,}\")\n",
        "\n",
        "# Test forward pass\n",
        "dummy_input = torch.randn(2, 3, 224, 224)\n",
        "output = model(dummy_input)\n",
        "print(f\"âœ“ Forward pass: {dummy_input.shape} â†’ {output.shape}\")\n",
        "\n",
        "# Test with contributions\n",
        "pred, features, contributions = model.forward_with_contributions(dummy_input)\n",
        "print(f\"âœ“ With contributions:\")\n",
        "print(f\"  Predictions: {pred.shape}\")\n",
        "print(f\"  Features: {features.shape}\")\n",
        "print(f\"  Contributions: {len(contributions)} feature NNs\")\n",
        "\n",
        "# Test parameter serialization\n",
        "params = model.get_parameters()\n",
        "print(f\"âœ“ Parameter serialization: {len(params)} parameters\")\n",
        "\n",
        "# Test baselines\n",
        "print(\"\\n2. Testing FedAvgCNN...\")\n",
        "baseline = FedAvgCNN(config)\n",
        "output = baseline(dummy_input)\n",
        "print(f\"âœ“ FedAvgCNN: {output.shape}\")\n",
        "\n",
        "print(\"\\n3. Testing CentralizedNAM...\")\n",
        "centralized = CentralizedNAM(config)\n",
        "output = centralized(dummy_input)\n",
        "print(f\"âœ“ CentralizedNAM: {output.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… TASK 3: MODELS MODULE - FULLY VALIDATED!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nðŸ“ Ready for Task 4: Training Module!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guVLf38OytPd",
        "outputId": "448de6d8-f861-4074-ddd2-2a481a1136da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fednams-plus\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 8 (delta 3), reused 8 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (8/8), 6.35 KiB | 433.00 KiB/s, done.\n",
            "From https://github.com/siddharth10ss/fednams-plus\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   2030c69..f7bd8b7  main       -> origin/main\n",
            "Updating 2030c69..f7bd8b7\n",
            "Fast-forward\n",
            " models/__init__.py          |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " models/baselines.py         | 236 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " models/feature_extractor.py | 149 \u001b[32m++++++++++++++++++++++++++++\u001b[m\n",
            " models/fednams_model.py     | 153 \u001b[32m++++++++++++++++++++++++++++\u001b[m\n",
            " models/nam_head.py          | 203 \u001b[32m+++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 5 files changed, 745 insertions(+), 2 deletions(-)\n",
            " create mode 100644 models/baselines.py\n",
            " create mode 100644 models/feature_extractor.py\n",
            " create mode 100644 models/fednams_model.py\n",
            " create mode 100644 models/nam_head.py\n",
            "ðŸ§ª Testing Models Module...\n",
            "\n",
            "1. Testing FedNAMsModel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 53.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model created\n",
            "  Parameters: 12,331,614\n",
            "âœ“ Forward pass: torch.Size([2, 3, 224, 224]) â†’ torch.Size([2, 15])\n",
            "âœ“ With contributions:\n",
            "  Predictions: torch.Size([2, 15])\n",
            "  Features: torch.Size([2, 512])\n",
            "  Contributions: 512 feature NNs\n",
            "âœ“ Parameter serialization: 3135 parameters\n",
            "\n",
            "2. Testing FedAvgCNN...\n",
            "âœ“ FedAvgCNN: torch.Size([2, 15])\n",
            "\n",
            "3. Testing CentralizedNAM...\n",
            "âœ“ CentralizedNAM: torch.Size([2, 15])\n",
            "\n",
            "============================================================\n",
            "âœ… TASK 3: MODELS MODULE - FULLY VALIDATED!\n",
            "============================================================\n",
            "\n",
            "ðŸ“ Ready for Task 4: Training Module!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHn1s6ZY5CKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}